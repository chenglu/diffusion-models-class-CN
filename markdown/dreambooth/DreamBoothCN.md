## (å¿…è¯») å‡†å¤‡å·¥ä½œ

- ç‚¹å‡»å³ä¸Šè§’ï¼ŒFork è¿™ä¸ª Notebook
- é¼ æ ‡æ‚¬åœåœ¨è¿è¡ŒæŒ‰é’®ï¼Œæˆ–è€…ç‚¹å‡»å³ä¸Šè§’é½¿è½®å›¾æ ‡è¿›å…¥é«˜çº§é€‰é¡¹ï¼Œç¡®ä¿ã€ŒæŒ‚è½½ work ç›®å½•ã€ä¸ºé€‰ä¸­çŠ¶æ€
- è¿è¡Œ Fork è¿‡çš„ Notebook

### å†æ¬¡ç¡®è®¤ã€ŒæŒ‚è½½ work ç›®å½•ã€å·²ç»å‹¾é€‰

![](https://devrel.andfun.cn/devrel/posts/2023/01/c0ecc61d04bf4.gif)


```python
# è¯·ç¡®ä¿ work ç›®å½•å­˜åœ¨ï¼Œä¸å­˜åœ¨çš„è¯ä»¥ä¸‹å‘½ä»¤ä¼šæŠ¥é”™ã€‚è¯·å…³é—­å®ä¾‹å†é‡æ–°åˆ›å»ºã€‚
! ls /home/mw/work
```

## Hugging Face DreamBooth ç¼–ç¨‹é©¬æ‹‰æ¾å¤§èµ› ğŸ†

åŸºäº HF diffusion class åˆ›ä½œã€‚

åŸæ–‡ https://github.com/huggingface/diffusion-models-class/tree/main/hackathon
è¯‘è€… SuSung-boy@ è‹æ¡‘ï¼Œç»å¸¸å€’è…¾å›¾åƒçš„å·¥ä¸šè§†è§‰ç®—æ³•å·¥ç¨‹å¸ˆã€‚

æ¬¢è¿æ¥åˆ° DreamBooth ç¼–ç¨‹é©¬æ‹‰æ¾ï¼åœ¨è¿™åœºæ¯”èµ›ä¸­ï¼Œæ‚¨å°†é€šè¿‡ **åœ¨å°‘é‡è‡ªå·±çš„å›¾åƒä¸Šè¿›è¡Œå¾®è°ƒæ¥å¾—åˆ°å…·æœ‰ä¸ªæ€§åŒ–çš„ Stable Diffusion æ¨¡å‹**ã€‚ä¸ºæ­¤ï¼Œæ‚¨å°†ä½¿ç”¨ä¸€é¡¹åä¸º [DreamBooth](https://arxiv.org/abs/2208.12242) çš„æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥å°†ä¸»ä½“ï¼ˆä¾‹å¦‚ï¼Œæ‚¨çš„å® ç‰©æˆ–å–œçˆ±çš„ç¾é£Ÿï¼‰æ¤å…¥æ¨¡å‹çš„è¾“å‡ºåŸŸï¼Œä»¥ä¾¿å¯ä»¥åœ¨æç¤ºä¸­ä½¿ç”¨ **å”¯ä¸€æ ‡è¯†ç¬¦** è¿›è¡Œåˆæˆã€‚

è®©æˆ‘ä»¬å¼€å§‹å§ï¼

ğŸš¨ **è¿›é˜¶ç”¨æˆ·æç¤º** 

æœ¬ Notebook æä¾›çš„ä»£ç æ˜¯ ğŸ¤— Diffusers ä¸­[å®˜æ–¹è®­ç»ƒè„šæœ¬](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth) çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œ**è¿è¡Œæœ¬ Notebook ä¸­çš„ä»£ç éœ€è¦è‡³å°‘ 14GB GPU vRAM**ã€‚è¯¥ä»£ç å·²ç»å¯ä»¥ä¸ºå¤§å¤šæ•°åº”ç”¨ç¨‹åºç”Ÿæˆä¸é”™çš„æ¨¡å‹ï¼Œä½†å¦‚æœæ‚¨æœ‰è¶…è¿‡ 24GB vRAM å¯ç”¨ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨å°è¯•é«˜çº§åŠŸèƒ½ï¼Œä¾‹å¦‚ class preservation æŸå¤±å’Œå¾®è°ƒæ–‡æœ¬ç¼–ç å™¨ã€‚ æŸ¥çœ‹ ğŸ¤— Diffusers [æ–‡æ¡£](https://hf.co/docs/diffusers/training/dreambooth) äº†è§£è¯¦æƒ…ã€‚

## DreamBooth æ˜¯ä»€ä¹ˆï¼Ÿ

DreamBooth æ˜¯ä¸€é¡¹ä½¿ç”¨ç‰¹å®šå½¢å¼çš„å¾®è°ƒæ¥å°†æ–°å¼•å…¥çš„æ¦‚å¿µä¼ æˆç»™ Stable Diffusion çš„æŠ€æœ¯ã€‚

Hugging Face çš„è¿™ç¯‡ [åšå®¢æ–‡ç« ](https://huggingface.co/blog/dreambooth) è¯´æ˜äº†ä½¿ç”¨ DreamBooth å¾®è°ƒ Stable Diffusion çš„ä¸€äº›æœ€ä½³å®è·µã€‚

DreamBooth çš„å·¥ä½œæ–¹å¼å¦‚ä¸‹ï¼š

* æ”¶é›†å¤§çº¦ 10-20 å¼ ç‰¹å®šä¸»ä½“ï¼ˆä¾‹å¦‚æ‚¨çš„ç‹—ç‹—ï¼‰çš„è¾“å…¥å›¾åƒï¼Œå¹¶å®šä¹‰ä¸€ä¸ªå”¯ä¸€æ ‡è¯†ç¬¦ [V]ï¼Œå®ƒä»£æŒ‡çš„å³æ˜¯æ‚¨è¾“å…¥çš„ä¸»ä½“ã€‚è¯¥æ ‡è¯†ç¬¦é€šå¸¸æ˜¯ä¸€äº›åƒ `flffydog` è¿™æ ·çš„è™šæ„è¯ï¼Œåœ¨æ¨ç†æ—¶å®ƒä¼šè¢«æ¤å…¥ä¸åŒçš„æ–‡æœ¬æç¤ºä¸­æ¥å°†ä¸»ä½“ç½®äºä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­ã€‚
* é€šè¿‡æä¾›å›¾åƒå’Œæ–‡æœ¬æç¤ºæ¥å¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œä¾‹å¦‚ç»™å®šæ–‡æœ¬æç¤º "A photo of a [V] dog"ï¼Œå…¶ä¸­éœ€åŒ…å«å”¯ä¸€æ ‡è¯†ç¬¦ [V] å’Œç±»åï¼ˆæœ¬ä¾‹ä¸­ä¸º "dog"ï¼‰
* ï¼ˆå¯é€‰ï¼‰é‡‡ç”¨ç‰¹æ®Šçš„ **class-specific prior preservation loss**, å®ƒåˆ©ç”¨äº†æ¨¡å‹åœ¨ç±»ä¸Šçš„è¯­ä¹‰å…ˆéªŒï¼Œå¹¶é€šè¿‡åœ¨æ–‡æœ¬æç¤ºä¸­æ³¨å…¥ç±»åæ¥ä¿ƒä½¿å®ƒç”Ÿæˆå±äºåŒä¸€ä¸»ä½“ç±»çš„å¤šç§å®ä¾‹ã€‚å®é™…ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œåªæœ‰äººè„¸åšä¸ºä¸»ä½“æ‰çœŸæ­£éœ€è¦æ­¤æ­¥éª¤ï¼Œè€Œå¯¹äºæ­¤æ¬¡ç¼–ç¨‹é©¬æ‹‰æ¾ä¸­è¦æ¢ç´¢çš„ä¸»é¢˜ï¼Œåˆ™å¯è·³è¿‡æ­¤æ­¥éª¤ã€‚

DreamBooth çš„æŠ€æœ¯æ¦‚è¿°å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![DreamBooth çš„æŠ€æœ¯æ¦‚è¿°](https://cdn.kesci.com/upload/image/ro83zspjq5.png?imageView2/0/w/960/h/960)

### DreamBooth èƒ½åšä»€ä¹ˆï¼Ÿ

é™¤äº†å°†æ‚¨çš„ä¸»ä½“æ”¾åœ¨æ„Ÿå…´è¶£çš„ä½ç½®ä¹‹å¤–ï¼ŒDreamBooth è¿˜å¯ç”¨äº _**æ–‡æœ¬å¼•å¯¼è§†å›¾åˆæˆ**_, æ‚¨å¯ä»¥ä»ä¸åŒçš„è§†è§’æŸ¥çœ‹ä¸»ä½“ï¼Œå¦‚ä¸‹ä¾‹æ‰€ç¤ºï¼š


![Image Name](https://cdn.kesci.com/upload/image/ro84168ko.png?imageView2/0/w/960/h/960)


DreamBooth è¿˜å¯ç”¨äºä¿®æ”¹ä¸»ä½“çš„å±æ€§ï¼Œä¾‹å¦‚é¢œè‰²æ”¹å˜å’ŒåŠ¨ç‰©æ··åˆï¼



![Image Name](https://cdn.kesci.com/upload/image/ro843np7oe.png?imageView2/0/w/960/h/960)


ä»¥ä¸Šå°±æ˜¯ä½¿ç”¨ DreamBooth åšçš„ä¸€äº›å¾ˆé…·çš„äº‹ï¼æ—¢ç„¶æˆ‘ä»¬å·²ç»å¤§è‡´äº†è§£äº†ï¼Œè®©æˆ‘ä»¬å¼€å§‹è®­ç»ƒè‡ªå·±çš„æ¨¡å‹å§ï¼

## ç¬¬ 1 æ­¥ï¼šè®¾ç½®

**æ–‡ç”Ÿå›¾çš„æ¨¡å‹ä¸€èˆ¬éƒ½å¾ˆå¤§**ï¼Œå› ä¸ºç½‘ç»œçš„åŸå› ä» HuggingFace ä¸»ç«™ç›´æ¥ä¸‹è½½é€Ÿåº¦æ¯”è¾ƒæ…¢ã€‚ä¸ºäº†æ–¹ä¾¿åŒå­¦ä»¬ä½¿ç”¨ï¼Œæˆ‘ä»¬å°†æ¨¡å‹æ–‡ä»¶åšæˆäº†å¯ä»¥ç›´æ¥åœ¨æœ¬åœ°æŒ‚è½½çš„ HeyWhale æ•°æ®é›†ã€‚

æŒ‚è½½æ•°æ®é›†åªéœ€ç‚¹å‡»å·¦è¾¹æ ç¬¬ä¸‰ä¸ªæ•°æ®é›†æŒ‰é’®ï¼Œæ‰“å¼€æŒ‚è½½æ•°æ®é¢æ¿ã€‚ç„¶åç‚¹å‡»â€œä¿®æ”¹â€ æŒ‰é’®ï¼Œé€‰ä¸­â€œä»–äººå…±äº«â€é‡Œé¢çš„ Hackathon æ•°æ®é›†ï¼Œç¡®å®šå³å¯ã€‚

æœ€ç»ˆç»“æœå¦‚å›¾æ‰€ç¤ºï¼š

*æ³¨æ„ï¼šå¦‚æœä½ æˆåŠŸæŒ‚è½½äº† work ç›®å½•ï¼Œè¿™äº›æ•°æ®é›†å°±é»˜è®¤æŒ‚è½½å®Œæˆï¼Œé€šå¸¸è¿™é‡Œæ— éœ€é¢å¤–æ“ä½œã€‚

![](https://cdn.kesci.com/upload/image/ro84bkw8in.png?imageView2/0/w/960/h/960)

è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æŒ‚è½½çš„æ•°æ®é›†æœ‰å“ªäº›æ–‡ä»¶ï¼Œæ‰§è¡Œä¸‹é¢çš„ä»£ç ï¼š


```python
%ls /home/mw/input/Hackathon6769/
```

    [0m[01;34mclip-vit-base-patch32[0m/            [01;34mstable-diffusion-v1-4[0m/
    [01;34mstable-diffusion-safety-checker[0m/


ç„¶åå®‰è£…æˆ‘ä»¬éœ€è¦çš„ä¾èµ–ï¼š


```python
%pip install -qqU diffusers transformers bitsandbytes accelerate ftfy datasets -i https://mirrors.cloud.tencent.com/pypi/simple 
```

    Note: you may need to restart the kernel to use updated packages.


ç„¶åå› ä¸ºå…¼å®¹æ€§çš„é—®é¢˜é™çº§ `ipywidgets` åˆ°æŸä¸€ä¸ªç‰¹å®šç‰ˆæœ¬ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯è‡ªå·±çš„æœºå™¨å¯èƒ½æ— éœ€è¿™ä¸€æ­¥æ“ä½œã€‚


```python
!pip install -qqU ipywidgets==7.6.3 -i https://mirrors.cloud.tencent.com/pypi/simple 
```

å®‰è£…å®Œæˆåé‡æ–°å¯åŠ¨ Kernelï¼Œç‚¹å‡»æœ¬ Notebook èœå•æ ä¸Šçš„é‡å¯ Kernel å³å¯:

![](https://devrel.andfun.cn/devrel/posts/2023/01/fbf8ac782e754.png)

æ¥ä¸‹æ¥è®©æˆ‘ä»¬ç™»å½• Hugging Face:


```python
%%capture
!sudo apt -qq install git-lfs
!git config --global credential.helper store
```


```python
from huggingface_hub import notebook_login
notebook_login()
```

    Token is valid.
    Your token has been saved in your configured git credential helpers (store).
    Your token has been saved to /home/mw/.huggingface/token
    Login successful


è¿™ä¸€æ­¥éœ€è¦å¤§å®¶è®¿é—® [Hugging Face çš„ Token è®¾ç½®](https://hf.co/settings/tokens) é¡µé¢ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªæœ‰å¯å†™ (WRITE) æƒé™çš„ token ï¼Œç„¶åç‚¹å‡»å³è¾¹æŒ‰é’®æŠŠå†…å®¹å¤åˆ¶è¿›æ¥ã€‚


![Image Name](https://cdn.kesci.com/upload/image/ro85de2kym.png?imageView2/0/w/960/h/960)


æœ€åè®©æˆ‘ä»¬å®šä¹‰ä¸€äº›åé¢ä¼šç”¨åˆ°çš„å¸¸é‡ã€‚


```python
MODEL_SD_PATH = '/home/mw/input/Hackathon6769/stable-diffusion-v1-4'
MODEL_CLIP_VIT_PATH = '/home/mw/input/Hackathon6769/clip-vit-base-patch32'
MODEL_SD_SAFETY_PATH = '/home/mw/input/Hackathon6769/stable-diffusion-safety-checker'
```

## ç¬¬ 2 æ­¥ï¼šé€‰æ‹©ä¸»é¢˜

æœ¬æ¬¡å¤§èµ›åŒ…å«5ä¸ª **ä¸»é¢˜** (theme)ï¼Œæ¯ä¸ªä¸»é¢˜å°†å¾é›†å±äºä»¥ä¸‹ç±»åˆ«çš„æ¨¡å‹ï¼š

- åŠ¨ç‰© ğŸ¨ (`animal`)ï¼š ä½¿ç”¨æ­¤ä¸»é¢˜ç”Ÿæˆæ‚¨çš„å® ç‰©æˆ–å–œçˆ±çš„åŠ¨ç‰©åœ¨é›…å…¸å«åŸæ¸¸ç©ã€åœ¨æ¸¸æ³³æˆ–åœ¨å¤ªç©ºä¸­é£è¡Œçš„å›¾åƒã€‚
- ç§‘å­¦ ğŸ”¬ (`science`)ï¼š ä½¿ç”¨æ­¤ä¸»é¢˜ç”Ÿæˆæ˜Ÿç³»ã€è›‹ç™½è´¨æˆ–ä»»ä½•è‡ªç„¶ç§‘å­¦å’ŒåŒ»å­¦é¢†åŸŸçš„é…·æ¯™çš„åˆæˆå›¾åƒã€‚
- é£Ÿç‰© ğŸ” (`food`)ï¼š ä½¿ç”¨æ­¤ä¸»é¢˜åœ¨æ‚¨æœ€å–œæ¬¢çš„ç¾å‘³ä½³è‚´å›¾åƒä¸Šå¾®è°ƒæ‚¨è‡ªå·±çš„ Stable Diffusionã€‚
- é£æ™¯ ğŸ” (`landscape`)ï¼š ä½¿ç”¨æ­¤ä¸»é¢˜ç”Ÿæˆæ‚¨æœ€å–œæ¬¢çš„å±±è„‰ã€æ¹–æ³Šæˆ–èŠ±å›­çš„ç¾ä¸½é£æ™¯å›¾åƒã€‚
- é€šç”¨ ğŸ”¥ (`wildcard`)ï¼š æ­¤ä¸»é¢˜æ— é™å®šçš„ç±»åˆ«ï¼Œæ‚¨å¯ä»¥ä¸ºé€‰æ‹©çš„ä»»ä½•ç±»åˆ«åˆ›å»º Stable Diffusion æ¨¡å‹ï¼

æˆ‘ä»¬å°†ä¸ºæ¯ä¸ªä¸»é¢˜çš„å‰ 3 åå–œçˆ±åº¦æœ€é«˜çš„æ¨¡å‹é¢å‘å¥–å“ï¼Œæˆ‘ä»¬é¼“åŠ±æ‚¨æäº¤å°½å¯èƒ½å¤šçš„æ¨¡å‹ï¼è¯·ä»ä»¥ä¸‹å‡ ä¸ªç±»åˆ«ä¸­é€‰ä¸€ä¸ªå§ã€‚


```python
# options=["animal", "science", "food", "landscape", "wildcard"],
options = "wildcard"
theme = options
```

## ç¬¬ 3 æ­¥ï¼šåˆ›å»ºå›¾åƒæ•°æ®é›†å¹¶ä¸Šä¼ åˆ° work ç›®å½•ä¸‹

é€‰å®šä¸»é¢˜åï¼Œä¸‹ä¸€æ­¥æ˜¯ **ä¸ºè¯¥ä¸»é¢˜åˆ›å»ºå›¾åƒæ•°æ®é›†** å¹¶å°†å…¶ä¸Šä¼ åˆ° work ç›®å½•ï¼š

* åœ¨ work ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªå­æ–‡ä»¶å¤¹ï¼Œç”¨äºå­˜æ”¾ç…§ç‰‡ï¼Œåç§°éšæ„ã€‚
* ç¡®å®šæ‚¨å¸Œæœ›æ¤å…¥æ¨¡å‹çš„ä¸»ä½“ï¼Œç„¶åéœ€è¦å‡†å¤‡å¤§çº¦ **10-20 å¼ ä¸»ä½“å›¾åƒ**ã€‚è¿™äº›å›¾åƒå¯ä»¥æ˜¯æ‚¨æ‹æ‘„çš„ç…§ç‰‡æˆ–ä» [Unsplash](https://unsplash.com/) ç­‰å¹³å°ä¸‹è½½çš„å›¾ç‰‡ã€‚æ›´æˆ–è€…ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹ Hugging Face Hub ä¸Šçš„ä»»ä½• [å›¾åƒæ•°æ®é›†](https://hf.co/datasets?task_categories=task_categories:image-classification&sort=downloads) æ¥è·å–çµæ„Ÿã€‚
* ä¸ºè·å¾—æœ€ä½³æ•ˆæœï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ **ä¸åŒè§’åº¦å’Œè§†è§’** æ‹æ‘„çš„ä¸»ä½“å›¾åƒ

åˆ°å·¦ä¾§è¾¹æ æ‰¾åˆ°ã€Œæ–‡ä»¶æ ‘ã€ï¼Œåœ¨ work ç›®å½•ä¸‹æ–°å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œä¸Šä¼ è‡ªå·±çš„ä¸»ä½“å›¾åƒç”¨äºå¾®è°ƒ:

![](https://devrel.andfun.cn/devrel/posts/2023/01/4c2f1c0fce0e6.gif)

æœ¬ demo é‡Œè®­ç»ƒçš„æ˜¯ Flutter çš„å‰ç¥¥ç‰© Dashï¼Œå› æ­¤æˆ‘åœ¨ `work` ç›®å½•ä¸‹åˆ›å»ºäº†ä¸€ä¸ª `dashdash` çš„æ–‡ä»¶å¤¹ã€‚


```python
# è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è¿™äº›ç…§ç‰‡ï¼Œdashdash æ˜¯æˆ‘çš„ dart ç©å¶çš„åå­—
! ls /home/mw/work/dashdash
```


```python
DATA_DIR = "/home/mw/work/dashdash"
```


```python
from datasets import load_dataset

dataset = load_dataset("imagefolder", data_dir=DATA_DIR)
dataset = dataset['train']
```

    Using custom data configuration default-ab024aecf581f3e7
    Found cached dataset imagefolder (/home/mw/.cache/huggingface/datasets/imagefolder/default-ab024aecf581f3e7/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)



      0%|          | 0/1 [00:00<?, ?it/s]



```python
# è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ç…§ç‰‡æ•°æ®æ˜¯å¦å·²ç»è½½å…¥ã€‚
dataset['image'][0]
```




<img src="https://cdn.kesci.com/upload/rt/619E565ED27D49C1A2EC1EE1E587FF98/ro94v7gjcd.png">



ç°åœ¨æˆ‘ä»¬æœ‰äº†è‡ªå·±çš„æ•°æ®é›†ï¼Œè®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°æ¥å¯è§†åŒ–æŸ¥çœ‹ä¸€äº›å›¾åƒï¼š


```python
from PIL import Image

def image_grid(imgs, rows, cols):
    assert len(imgs) == rows * cols
    w, h = imgs[0].size
    grid = Image.new("RGB", size=(cols * w, rows * h))
    grid_w, grid_h = grid.size
    for i, img in enumerate(imgs):
        grid.paste(img, box=(i % cols * w, i // cols * h))
    return grid

num_samples = 4
image_grid(dataset["image"][:num_samples], rows=1, cols=num_samples)
```

è¿™äº›å›¾åƒå¦‚æœçœ‹èµ·æ¥ä¸é”™ï¼Œæ‚¨å¯ä»¥ç»§ç»­ä¸‹ä¸€æ­¥ â€”â€” åˆ›å»º PyTorch æ•°æ®é›†ä»¥ä½¿ç”¨ DreamBooth è¿›è¡Œè®­ç»ƒã€‚

### åˆ›å»ºè®­ç»ƒæ•°æ®é›†

è¦ä¸ºæˆ‘ä»¬çš„å›¾åƒåˆ›å»ºè®­ç»ƒé›†ï¼Œéœ€è¦ä¸€äº›ç»„ä»¶ï¼š

* **å®ä¾‹æç¤º** : ç”¨äºåœ¨è®­ç»ƒå¼€å§‹æ—¶é¢„çƒ­æ¨¡å‹ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä½¿ç”¨ä¸€å¼ ã€Œæ ‡è¯†ç¬¦ + ç±»åˆ«åè¯ã€çš„ç…§ç‰‡æ•ˆæœå·²è¶³å¤Ÿå¥½ï¼Œä¾‹å¦‚ä¸ºæˆ‘ä»¬å¯çˆ±çš„æŸ¯åŸºå›¾ç‰‡å†™ä¸Šæç¤º: "ä¸€å¼ æŸ¯æŸ¯åŸºç‹—çš„ç…§ç‰‡"ã€‚
    * **æ³¨æ„ï¼š** å»ºè®®æ‚¨é€‰æ‹©ä¸€ä¸ªç‹¬ç‰¹çš„/è™šæ„è¯æ¥æè¿°æ‚¨çš„ä¸»ä½“ï¼Œå¦‚ `æŸ¯æŸ¯åŸº`ã€‚ä»¥æ­¤ç¡®ä¿æ¨¡å‹è¯æ±‡è¡¨ä¸­çš„å¸¸ç”¨è¯ä¸ä¼šè¢«è¦†ç›–ã€‚
* **åˆ†è¯å™¨** : ç”¨äºå°†å®ä¾‹æç¤ºè½¬æ¢ä¸ºè¾“å…¥ IDï¼Œå¹¶ä¸”å¯ä»¥å°†å…¶æä¾›ç»™ Stable Diffusion çš„æ–‡æœ¬ç¼–ç å™¨ã€‚
* ä¸€ç»„ **å›¾åƒå˜æ¢** : å°¤å…¶æ˜¯å°†å›¾åƒç¼©æ”¾è‡³é€šç”¨å½¢çŠ¶ï¼Œä»¥åŠå°†åƒç´ å€¼å½’ä¸€åŒ–è‡³é€šç”¨å‡å€¼å’Œæ ‡å‡†åˆ†å¸ƒã€‚

æ ¹æ®ä»¥ä¸Šæè¿°ï¼Œè®©æˆ‘ä»¬ä»å®šä¹‰å®ä¾‹æç¤ºå¼€å§‹ï¼š


```python
name_of_your_concept = "dashdash"  # æ ¹æ®æ‚¨çš„ä¸»ä½“ä¿®æ”¹ï¼Œæˆ‘è¿™é‡ŒæŠŠ dash ç§°ä¹‹ä¸º dashdash
type_of_thing = "toy"  # æ ¹æ®æ‚¨çš„ä¸»ä½“ä¿®æ”¹ 
instance_prompt = f"a photo of {name_of_your_concept} {type_of_thing}"
print(f"Instance prompt: {instance_prompt}")
```

    Instance prompt: a photo of dashdash toy


æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª PyTorch `Dataset` ç±», å¹¶å®ç° `__len__` å’Œ `__getitem__` æ–¹æ³•ï¼š


```python
from torch.utils.data import Dataset
from torchvision import transforms


class DreamBoothDataset(Dataset):
    def __init__(self, dataset, instance_prompt, tokenizer, size=512):
        self.dataset = dataset
        self.instance_prompt = instance_prompt
        self.tokenizer = tokenizer
        self.size = size
        self.transforms = transforms.Compose(
            [
                transforms.Resize(size),
                transforms.CenterCrop(size),
                transforms.ToTensor(),
                transforms.Normalize([0.5], [0.5]),
            ]
        )

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, index):
        example = {}
        image = self.dataset[index]["image"]
        example["instance_images"] = self.transforms(image)
        example["instance_prompt_ids"] = self.tokenizer(
            self.instance_prompt,
            padding="do_not_pad",
            truncation=True,
            max_length=self.tokenizer.model_max_length,
        ).input_ids
        return example
```

å¾ˆå¥½ï¼Œç°åœ¨è®©æˆ‘ä»¬åŠ è½½ä¸åŸå§‹ Stable Diffusion æ¨¡å‹çš„æ–‡æœ¬ç¼–ç å™¨å…³è”çš„ CLIP åˆ†è¯å™¨ï¼Œç„¶åæ£€æŸ¥ä¸Šä¸€æ­¥æ˜¯å¦ç”Ÿæ•ˆï¼Œç„¶ååˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼š


```python
from transformers import CLIPTokenizer

# ç”¨æ¥å¾®è°ƒçš„ Stable Diffusion æ£€æŸ¥ç‚¹
model_id = MODEL_SD_PATH
tokenizer = CLIPTokenizer.from_pretrained(
    model_id,
    subfolder="tokenizer",
)

train_dataset = DreamBoothDataset(dataset, instance_prompt, tokenizer)
train_dataset[0]
```




    {'instance_images': tensor([[[ 0.6941,  0.6941,  0.6941,  ...,  0.3647,  0.3647,  0.3647],
              [ 0.6941,  0.7020,  0.6941,  ...,  0.3647,  0.3647,  0.3647],
              [ 0.6863,  0.6863,  0.6863,  ...,  0.3647,  0.3647,  0.3569],
              ...,
              [ 0.5216,  0.5294,  0.5216,  ...,  0.5529,  0.5451,  0.5451],
              [ 0.5216,  0.5216,  0.5294,  ...,  0.5529,  0.5451,  0.5529],
              [ 0.5216,  0.5373,  0.5373,  ...,  0.5451,  0.5529,  0.5608]],
     
             [[ 0.5529,  0.5529,  0.5529,  ..., -0.0275, -0.0275, -0.0275],
              [ 0.5529,  0.5608,  0.5529,  ..., -0.0275, -0.0275, -0.0275],
              [ 0.5451,  0.5451,  0.5451,  ..., -0.0275, -0.0275, -0.0353],
              ...,
              [ 0.5216,  0.5294,  0.5216,  ...,  0.5608,  0.5529,  0.5529],
              [ 0.5216,  0.5216,  0.5294,  ...,  0.5608,  0.5529,  0.5608],
              [ 0.5216,  0.5373,  0.5373,  ...,  0.5529,  0.5608,  0.5686]],
     
             [[ 0.6784,  0.6784,  0.6784,  ..., -0.6000, -0.6000, -0.6000],
              [ 0.6784,  0.6863,  0.6784,  ..., -0.5922, -0.5922, -0.5922],
              [ 0.6706,  0.6706,  0.6706,  ..., -0.5843, -0.5843, -0.5922],
              ...,
              [ 0.5843,  0.5922,  0.5843,  ...,  0.5765,  0.5686,  0.5686],
              [ 0.5843,  0.5843,  0.5922,  ...,  0.5765,  0.5686,  0.5765],
              [ 0.5843,  0.6000,  0.6000,  ...,  0.5686,  0.5765,  0.5843]]]),
     'instance_prompt_ids': [49406, 320, 1125, 539, 13858, 10206, 5988, 49407]}



## ç¬¬ 4 æ­¥ï¼šå®šä¹‰æ•°æ®æ•´ç†å™¨

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªæ•°æ®æ•´ç†å™¨ã€‚æ•°æ®æ•´ç†å™¨æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå®ç°çš„åŠŸèƒ½æ˜¯ï¼šæ”¶é›†ä¸€ä¸ªæ‰¹æ¬¡æ•°æ®ä¸­çš„å…ƒç´ ã€åº”ç”¨ä¸€äº›é€»è¾‘æ¥æ„æˆå•ä¸ªå¼ é‡ã€æä¾›ç»™æ¨¡å‹è®­ç»ƒç­‰ã€‚å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥è§‚çœ‹ [Hugging Face çš„è§†é¢‘è¯¾ç¨‹](hf.co/course)ã€‚

å¯¹äº DreamBoothï¼Œæˆ‘ä»¬å®šä¹‰çš„æ•°æ®æ•´ç†å™¨éœ€è¦ä¸ºæ¨¡å‹æä¾›ä¸¤ä¸ªéƒ¨åˆ†ï¼šåˆ†è¯å™¨çš„è¾“å…¥ IDã€å›¾åƒçš„åƒç´ å€¼å †å å¼ é‡ã€‚å…·ä½“å‡½æ•°ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š


```python
import torch

def collate_fn(examples):
    input_ids = [example["instance_prompt_ids"] for example in examples]
    pixel_values = [example["instance_images"] for example in examples]
    pixel_values = torch.stack(pixel_values)
    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()

    input_ids = tokenizer.pad(
        {"input_ids": input_ids}, padding=True, return_tensors="pt"
    ).input_ids

    batch = {
        "input_ids": input_ids,
        "pixel_values": pixel_values,
    }
    return batch
```

## ç¬¬ 5 æ­¥ï¼šåŠ è½½ Stable Diffusion ç®¡é“ç»„ä»¶

åˆ°æ­¤æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½è®­ç»ƒæ‰€éœ€çš„å¤§éƒ¨åˆ†ç»„ä»¶äº†ï¼å¦‚ Stable Diffusion ç¬¬ 3 å•å…ƒ Notebook ä¸­æ‰€ç¤ºï¼Œä¸€ä¸ªç®¡é“åŒ…å«å¤šä¸ªæ¨¡å‹ï¼š

* æ–‡æœ¬ç¼–ç å™¨: ç”¨äºå°†æ–‡æœ¬æç¤ºè½¬æ¢ä¸ºåµŒå…¥çŸ©é˜µã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ CLIPï¼Œå› ä¸ºå®ƒæ˜¯ç”¨äºè®­ç»ƒ Stable Diffusion v1-4 çš„ç¼–ç å™¨
* VAE (å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨, variational autoencoder): ç”¨äºå°†å›¾åƒè½¬æ¢ä¸ºå‹ç¼©è¡¨å¾ï¼ˆéšå¼è¡¨å¾ï¼‰ï¼Œå¹¶åœ¨æ¨ç†æ—¶è§£å‹ç¼©
* UNet: ç”¨äºåœ¨éšå¼ VAE ä¸­å»å™ª

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ ğŸ¤— Diffusers å’Œ ğŸ¤— Transformers åº“åŠ è½½ä¸Šè¿°æ‰€æœ‰ç»„ä»¶ï¼Œå¦‚ä¸‹ä»£ç æ‰€ç¤ºï¼š



```python
from diffusers import AutoencoderKL, UNet2DConditionModel
from transformers import CLIPFeatureExtractor, CLIPTextModel

text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder="text_encoder")
vae = AutoencoderKL.from_pretrained(model_id, subfolder="vae")
unet = UNet2DConditionModel.from_pretrained(model_id, subfolder="unet")
feature_extractor = CLIPFeatureExtractor.from_pretrained(MODEL_CLIP_VIT_PATH)
```

## ç¬¬ 6 æ­¥ï¼šå¾®è°ƒæ¨¡å‹

æœ‰è¶£çš„ä¸€æ­¥æ¥äº†ï¼ä½¿ç”¨ DreamBooth è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ï¼å¦‚ [Hugging Face çš„åšå®¢æ–‡ç« ](https://huggingface.co/blog/dreambooth) æ‰€æè¿°çš„é‚£æ ·ï¼Œéœ€è¦æ‰‹åŠ¨è°ƒæ•´çš„æœ€é‡è¦çš„è¶…å‚æ•°æ˜¯å­¦ä¹ ç‡å’Œè®­ç»ƒæ¬¡æ•°ã€‚

é€šå¸¸ï¼Œè¾ƒä½çš„å­¦ä¹ ç‡å’Œè¾ƒé•¿çš„è®­ç»ƒæ¬¡æ•°å¯ä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚ä¸‹é¢è®¾ç½®çš„åˆå§‹å€¼æ˜¯ä¸€ä¸ªä¸é”™çš„è®­ç»ƒèµ·ç‚¹ï¼Œä½†æ‚¨å¯èƒ½ä»ç„¶éœ€è¦æ ¹æ®æ‚¨çš„æ•°æ®é›†è°ƒæ•´å®ƒä»¬ï¼š


```python
learning_rate = 2e-06
max_train_steps = 400
```

æ¥ä¸‹æ¥ï¼Œå°†è®­ç»ƒéœ€è¦çš„å…¶ä»–è¶…å‚æ•°åŒ…è£…åœ¨ `Namespace` å¯¹è±¡ä¸­ï¼Œæ¥ä½¿é…ç½®å’Œè®­ç»ƒæ›´ç®€å•ï¼š


```python
from argparse import Namespace

OUTPUT_DIR = "/home/mw/work/my-dreambooth"
args = Namespace(
    pretrained_model_name_or_path=model_id,
    resolution=512, # Reduce this if you want to save some memory
    train_dataset=train_dataset,
    instance_prompt=instance_prompt,
    learning_rate=learning_rate,
    max_train_steps=max_train_steps,
    train_batch_size=1,
    gradient_accumulation_steps=1, # Increase this if you want to lower memory usage
    max_grad_norm=1.0,
    gradient_checkpointing=True,  # set this to True to lower the memory usage.
    use_8bit_adam=True,  # use 8bit optimizer from bitsandbytes
    seed=3434554,
    sample_batch_size=2,
    output_dir=OUTPUT_DIR,  # where to save the pipeline
)
```

æœ€åè¦å®šä¹‰ä¸€ä¸ª `training_function()` å‡½æ•°ï¼Œå®ƒåŒ…è£…äº†ä¸€äº›è®­ç»ƒé€»è¾‘ï¼Œå¹¶ä¸”å¯ä»¥ä¼ é€’ç»™ ğŸ¤— Accelerate åº“æ¥å¤„ç† 1 ä¸ªæˆ–å¤šä¸ª GPU ä¸Šçš„è®­ç»ƒã€‚å¦‚æœè¿™æ˜¯æ‚¨ç¬¬ä¸€æ¬¡ä½¿ç”¨ ğŸ¤— Accelerateï¼Œè¯·è§‚çœ‹æˆ‘ä»¬å®˜æ–¹çš„ Bilibili é¢‘é“è§†é¢‘ä»¥å¿«é€Ÿäº†è§£å®ƒçš„åŠŸèƒ½ï¼š[Supercharge your PyTorch training loop with Accelerate](https://www.bilibili.com/video/BV1gD4y157ee/) ï¼ˆå¸¦ä¸­è‹±æ–‡å­—å¹•ï¼‰ã€‚


å½“æˆ‘ä»¬ä»å¤´å¼€å§‹è®­ç»ƒè‡ªå·±çš„æ‰©æ•£æ¨¡å‹æ—¶ï¼Œè¿™äº›ç»†èŠ‚ä¸æˆ‘ä»¬åœ¨ç¬¬ 1 å’Œç¬¬ 2 å•å…ƒä¸­çœ‹åˆ°çš„ç±»ä¼¼ï¼š


```python
import math

import torch.nn.functional as F
from accelerate import Accelerator
from accelerate.utils import set_seed
from diffusers import DDPMScheduler, PNDMScheduler, StableDiffusionPipeline
from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
from torch.utils.data import DataLoader
from tqdm.auto import tqdm


def training_function(text_encoder, vae, unet):

    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
    )

    set_seed(args.seed)

    if args.gradient_checkpointing:
        unet.enable_gradient_checkpointing()

    # ä½¿ç”¨ 8 ä½ Adam ä¼˜åŒ–å™¨ä»¥é™ä½å†…å­˜å ç”¨ï¼Œæˆ–è€…åœ¨ 16GB GPU å¾®è°ƒæ¨¡å‹
    if args.use_8bit_adam:
        import bitsandbytes as bnb
        optimizer_class = bnb.optim.AdamW8bit
    else:
        optimizer_class = torch.optim.AdamW

    optimizer = optimizer_class(
        unet.parameters(),  # ä»…ä¼˜åŒ– UNet
        lr=args.learning_rate,
    )

    noise_scheduler = DDPMScheduler(
        beta_start=0.00085,
        beta_end=0.012,
        beta_schedule="scaled_linear",
        num_train_timesteps=1000,
    )

    train_dataloader = DataLoader(
        args.train_dataset,
        batch_size=args.train_batch_size,
        shuffle=True,
        collate_fn=collate_fn,
    )

    unet, optimizer, train_dataloader = accelerator.prepare(
        unet, optimizer, train_dataloader
    )

    # å°† text_encode å’Œ VAE è½¬ç§»åˆ° gpu
    text_encoder.to(accelerator.device)
    vae.to(accelerator.device)

    # æˆ‘ä»¬éœ€è¦é‡æ–°è®¡ç®—æˆ‘ä»¬çš„æ€»è®­ç»ƒæ¬¡æ•°ï¼Œå› ä¸ºæ•°æ®åŠ è½½å™¨çš„å¤§å°å¯èƒ½å‘ç”Ÿæ”¹å˜ã€‚
    num_update_steps_per_epoch = math.ceil(
        len(train_dataloader) / args.gradient_accumulation_steps
    )
    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

    # è®­ç»ƒ!
    total_batch_size = (
        args.train_batch_size
        * accelerator.num_processes
        * args.gradient_accumulation_steps
    )
    # æ¯å°æœºå™¨ä»…æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦æ¡
    progress_bar = tqdm(
        range(args.max_train_steps), disable=not accelerator.is_local_main_process
    )
    progress_bar.set_description("Steps")
    global_step = 0

    for epoch in range(num_train_epochs):
        unet.train()
        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(unet):
                # è½¬æ¢å›¾åƒè‡³éšå¼ç©ºé—´
                with torch.no_grad():
                    latents = vae.encode(batch["pixel_values"]).latent_dist.sample()
                    latents = latents * 0.18215

                # é‡‡æ ·è¦æ·»åŠ åˆ°éšå¼ç©ºé—´çš„å™ªå£°æ ·æœ¬
                noise = torch.randn(latents.shape).to(latents.device)
                bsz = latents.shape[0]
                # ä¸ºæ¯å¼ å›¾åƒé‡‡æ ·éšæœºæ—¶é—´æ­¥
                timesteps = torch.randint(
                    0,
                    noise_scheduler.config.num_train_timesteps,
                    (bsz,),
                    device=latents.device,
                ).long()

                # æ ¹æ®æ¯ä¸ªæ—¶é—´æ­¥çš„å™ªå£°å¹…åº¦ï¼Œå°†å™ªå£°æ·»åŠ åˆ°éšå¼ç©ºé—´
                # (å³å‰å‘æ‰©æ•£è¿‡ç¨‹)
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

                # è·å–ç”¨äºæ¡ä»¶è°ƒèŠ‚çš„æ–‡æœ¬åµŒå…¥
                with torch.no_grad():
                    encoder_hidden_states = text_encoder(batch["input_ids"])[0]

                # é¢„æµ‹å™ªå£°æ®‹å·®
                noise_pred = unet(
                    noisy_latents, timesteps, encoder_hidden_states
                ).sample
                loss = (
                    F.mse_loss(noise_pred, noise, reduction="none")
                    .mean([1, 2, 3])
                    .mean()
                )

                accelerator.backward(loss)
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)
                optimizer.step()
                optimizer.zero_grad()

            # æ£€æŸ¥åŠ é€Ÿå™¨æ˜¯å¦åœ¨å¹•åæ‰§è¡Œäº†ä¼˜åŒ–
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1

            logs = {"loss": loss.detach().item()}
            progress_bar.set_postfix(**logs)

            if global_step >= args.max_train_steps:
                break

        accelerator.wait_for_everyone()

    # ä½¿ç”¨ç»è®­ç»ƒçš„æ¨¡å—ï¼Œåˆ›å»ºç®¡é“å¹¶ä¿å­˜ã€‚
    if accelerator.is_main_process:
        print(f"Loading pipeline and saving to {args.output_dir}...")
        scheduler = PNDMScheduler(
            beta_start=0.00085,
            beta_end=0.012,
            beta_schedule="scaled_linear",
            skip_prk_steps=True,
            steps_offset=1,
        )
        pipeline = StableDiffusionPipeline(
            text_encoder=text_encoder,
            vae=vae,
            unet=accelerator.unwrap_model(unet),
            tokenizer=tokenizer,
            scheduler=scheduler,
            safety_checker=StableDiffusionSafetyChecker.from_pretrained(
                MODEL_SD_SAFETY_PATH
            ),
            feature_extractor=feature_extractor,
        )
        pipeline.save_pretrained(args.output_dir)
```

ç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰å‡½æ•°ï¼Œå¼€å§‹è®­ç»ƒå§ï¼æ ¹æ®æ‚¨çš„æ•°æ®é›†çš„å¤§å°å’Œ GPU çš„ç±»å‹ï¼Œå¯èƒ½éœ€è¦ 5 åˆ†é’Ÿåˆ° 1 å°æ—¶ä¸ç­‰çš„æ—¶é—´æ‰èƒ½è¿è¡Œï¼š


```python
from accelerate import notebook_launcher

num_of_gpus = 1  # ä»¥åŠæ‚¨æ‹¥æœ‰çš„ GPU æ•°é‡ä¿®æ”¹æ­¤é¡¹
notebook_launcher(
    training_function, args=(text_encoder, vae, unet), num_processes=num_of_gpus
)
```

    Launching training on one GPU.
    
    ===================================BUG REPORT===================================
    Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
    For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link
    ================================================================================
    CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...
    CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so
    CUDA SETUP: Highest compute capability among GPUs detected: 7.5
    CUDA SETUP: Detected CUDA version 116
    CUDA SETUP: Loading binary /opt/conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...


    /opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:134: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}
      warn(msg)
    /opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:134: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...
      warn(msg)
    /opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:134: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.16.252.1'), PosixPath('tcp'), PosixPath('443')}
      warn(msg)
    /opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:134: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}
      warn(msg)



      0%|          | 0/400 [00:00<?, ?it/s]


å¦‚æœæ‚¨åœ¨å•ä¸ª GPU ä¸Šè¿è¡Œï¼Œæ‚¨å¯ä»¥å°†ä¸‹é¢çš„ä»£ç å¤åˆ¶åˆ°ä¸€ä¸ªæ–°çš„å•å…ƒæ ¼å¹¶è¿è¡Œæ¥ä¸ºä¸‹ä¸€éƒ¨åˆ†é‡Šæ”¾ä¸€äº›å†…å­˜ã€‚å¯¹äºå¤š GPU æœºå™¨ï¼ŒğŸ¤— Accelerate ä¸å…è®¸ _ä»»ä½•_ å•å…ƒæ ¼ç›´æ¥ä½¿ç”¨ torch.cuda è®¿é—® GPUï¼Œå› æ­¤æˆ‘ä»¬ä¸å»ºè®®åœ¨è¿™äº›æƒ…å†µä¸‹ä½¿ç”¨æ­¤æŠ€å·§ï¼š

```python
with torch.no_grad():
    torch.cuda.empty_cache()
```

## ç¬¬ 7 æ­¥ï¼šè¿è¡Œæ¨ç†å¹¶æ£€æŸ¥ç”Ÿæˆ

ç°åœ¨æˆ‘ä»¬çš„æ¨¡å‹å·²ç»è®­ç»ƒå®Œæ¯•ï¼Œè®©æˆ‘ä»¬ç”¨å®ƒç”Ÿæˆä¸€äº›å›¾åƒï¼Œçœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ï¼é¦–å…ˆï¼Œæˆ‘ä»¬è¦ä»æ¨¡å‹ä¿å­˜ç›®å½•åŠ è½½ç®¡é“ï¼š


```python
pipe = StableDiffusionPipeline.from_pretrained(
    args.output_dir,
    torch_dtype=torch.float16,
).to("cuda")
```

æ¥ä¸‹æ¥è®©æˆ‘ä»¬å°è¯•ç”Ÿæˆä¸€äº›å›¾åƒã€‚åœ¨æŠ±æŠ±è„¸ Hub å°éƒ¨ä»¶ä¸Šä¸º `prompt` å˜é‡è®¾ç½®é»˜è®¤å€¼ï¼Œå¯ä»¥ç¨å¾®è¯•éªŒå‡ æ¬¡æ¥æ‰¾åˆ°ä¸€ä¸ªè¾ƒå¥½çš„å€¼ã€‚å¦‚æœæ‚¨è¿˜æƒ³å°è¯•ä½¿ç”¨ [CLIP Interrogator](https://huggingface.co/spaces/pharma/CLIP-Interrogator) åˆ›å»ºæ›´è¯¦ç»†çš„æç¤ºï¼Œè¯·å‚è€ƒä¸‹æ–‡ï¼š


```python
# Pick a funny prompt here and it will be used as the widget's default 
# when we push to the Hub in the next section
prompt = f"illustration of a dashdash toy sitting on top of the deck of a battle ship traveling through the open sea with a lot of ships surrounding it"

# Tune the guidance to control how closely the generations follow the prompt.
# Values between 7-11 usually work best
guidance_scale = 7

num_cols = 2
all_images = []
for _ in range(num_cols):
    images = pipe(prompt, guidance_scale=guidance_scale).images
    all_images.extend(images)

image_grid(all_images, 1, num_cols)
```

## ç¬¬ 8 æ­¥ï¼šå°†æ‚¨çš„æ¨¡å‹æ¨é€åˆ° Hub

å¦‚æœæ‚¨è§‰å¾—è‡ªå·±çš„æ¨¡å‹éå¸¸æ£’ï¼Œæœ€åä¸€æ­¥æ˜¯å°†å…¶æ¨é€åˆ° Hub å¹¶åœ¨ [DreamBooth æ’è¡Œæ¦œ](https://huggingface.co/spaces/dreambooth-hackathon/leaderboard)ä¸ŠæŸ¥çœ‹ï¼

âš ï¸ ç”±äºç½‘ç»œåŸå› ï¼Œè¿™ä¸€æ­¥å¯èƒ½ä¼šèŠ±è´¹å‡ åˆ†é’Ÿã€‚å¦‚æœå¤±è´¥ï¼Œè¯·é‡è¯•ã€‚

é¦–å…ˆï¼Œæ‚¨éœ€è¦ä¸ºæ¨¡å‹åº“èµ·ä¸€ä¸ªåå­—ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å”¯ä¸€æ ‡è¯†ç¬¦å’Œç±»åæ¥å‘½åï¼Œä½†å¦‚æœæ‚¨æ„¿æ„ï¼Œå¯ä»¥éšæ—¶æ›´æ”¹ï¼š


```python
# åœ¨ Hub ä¸Šä¸ºæ‚¨çš„æ¨¡å‹èµ·ä¸€ä¸ªåå­—ã€‚ä¸å…è®¸åŒ…å«ç©ºæ ¼ã€‚
model_name = f"{name_of_your_concept}-{type_of_thing}-heywhale"
```

æ¥ä¸‹æ¥ï¼Œæ·»åŠ ä¸€ä¸ªç®€çŸ­æè¿°ï¼Œä»‹ç»ä¸€ä¸‹æ‚¨è®­ç»ƒçš„æ¨¡å‹ç±»å‹æˆ–è€…æƒ³è¦åˆ†äº«çš„ä»»ä½•å…¶ä»–ä¿¡æ¯ï¼š


```python
# æè¿°ä¸€ä¸‹æ‚¨é€‰æ‹©çš„ä¸»é¢˜ä»¥åŠè®­ç»ƒå¥½çš„æ¨¡å‹
description = f"""
This is a Stable Diffusion model fine-tuned on `{type_of_thing}` images for the {theme} theme, 
for the Hugging Face DreamBooth Hackathon, from the HF CN Community, 
corporated with the HeyWhale.
"""

print(description)
```

    
    This is a Stable Diffusion model fine-tuned on `toy` images for the wildcard theme, 
    for the Hugging Face DreamBooth Hackathon, from the HF CN Community, 
    corporated with the HeyWhale.
    


æœ€åï¼Œè¿è¡Œä¸‹é¢çš„å•å…ƒæ ¼åœ¨ Hub ä¸Šåˆ›å»ºä¸€ä¸ª repoï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªç²¾ç¾çš„æ¨¡å‹å¡ï¼ŒåŒæ—¶å°†æ‰€æœ‰çš„æ–‡ä»¶æ¨é€åˆ°å¼•å¯¼ï¼š


```python
# å°†æœ¬åœ°ä¿å­˜çš„ç®¡é“ä¸Šä¼ åˆ° Hub çš„ä»£ç 
from huggingface_hub import HfApi, ModelCard, create_repo, get_full_repo_name

# åˆ›å»ºåº“
hub_model_id = get_full_repo_name(model_name)
create_repo(hub_model_id)
```


```python
# ä¸Šä¼ æ–‡ä»¶
api = HfApi()
api.upload_folder(folder_path=args.output_dir, path_in_repo="", repo_id=hub_model_id)
```


```python
# æ·»åŠ  metadata
content = f"""
---
license: creativeml-openrail-m
tags:
- pytorch
- diffusers
- stable-diffusion
- text-to-image
- diffusion-models-class
- dreambooth-hackathon
- {theme}
widget:
- text: {prompt}
---

# DreamBooth model for the {name_of_your_concept} concept trained by {api.whoami()["name"]}.

This is a Stable Diffusion model fine-tuned on the {name_of_your_concept} concept with DreamBooth. It can be used by modifying the `instance_prompt`: **{instance_prompt}**

This model was created as part of the DreamBooth Hackathon ğŸ”¥. Visit the [organisation page](https://huggingface.co/dreambooth-hackathon) for instructions on how to take part!

## Description

{description}

## Usage

```python
from diffusers import StableDiffusionPipeline

pipeline = StableDiffusionPipeline.from_pretrained('{hub_model_id}')
image = pipeline().images[0]
image
```
"""

card = ModelCard(content)
hub_url = card.push_to_hub(hub_model_id)
print(f"Upload successful! Model can be found here: {hub_url}")
print(
    f"View your submission on the public leaderboard here: https://huggingface.co/spaces/dreambooth-hackathon/leaderboard"
)
```

æ­å–œæ­å–œ ğŸ‰ ä½ å·²ç»è®­ç»ƒäº†ä¸€ä¸ªè‡ªå·±çš„æ–‡ç”Ÿå›¾æ¨¡å‹ï¼Œå¹¶ä¸”æˆåŠŸä¸Šä¼ åˆ°äº† HuggingFaceï¼å¿«å»è·Ÿæœ‹å‹ä»¬åˆ†äº«ï¼Œè¯·ä»–ä»¬ä½¿ç”¨å’Œç‚¹èµå§ï½

## ä¸‹ä¸€æ­¥

æäº¤å‚èµ›ä½œå“ï¼Œè¯·åœ¨ [è¿™é‡Œ](https://www.heywhale.com/org/HuggingFace/competition/area/63bbfb98de6c0e9cdb0d9dd5/submit) æäº¤å·²ç»ä¸Šä¼ åˆ° Hugging Face ä¸Šçš„æ¨¡å‹ URLã€‚

