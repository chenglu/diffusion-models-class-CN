# åˆ›å»ºä¸€ä¸ªç±»åˆ«æ¡ä»¶æ‰©æ•£æ¨¡å‹

åœ¨è¿™èŠ‚ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†é˜è¿°ä¸€ç§ç»™æ‰©æ•£æ¨¡å‹åŠ æ¡ä»¶ä¿¡æ¯çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ¥ç€[è¿™ä¸ªä»å¤´è®­ç»ƒçš„ä¾‹å­](../unit1/02_diffusion_models_from_scratch_CN.ipynb)åœ¨ MNIST ä¸Šè®­ç»ƒä¸€ä¸ªä»¥ç±»åˆ«ä¸ºæ¡ä»¶çš„æ‰©æ•£æ¨¡å‹ã€‚è¿™é‡Œæˆ‘ä»¬å¯ä»¥åœ¨æ¨ç†æ—¶æŒ‡å®šæˆ‘ä»¬è¦ç”Ÿæˆçš„æ˜¯å“ªä¸ªæ•°å­—ã€‚

å°±åƒæœ¬å•å…ƒä»‹ç»ä¸­è¯´çš„é‚£æ ·ï¼Œè¿™åªæ˜¯å¾ˆå¤šç»™æ‰©æ•£æ¨¡å‹æ·»åŠ é¢å¤–æ¡ä»¶ä¿¡æ¯çš„æ–¹æ³•ä¸­çš„ä¸€ç§ï¼Œè¿™é‡Œç”¨å®ƒåšç¤ºä¾‹æ˜¯å› ä¸ºå®ƒæ¯”è¾ƒç®€å•ã€‚å°±åƒç¬¬ä¸€å•å…ƒä¸­â€œä»èµ°è®­ç»ƒâ€çš„ä¾‹å­ä¸€æ ·ï¼Œè¿™èŠ‚ç¬”è®°æœ¬ä¹Ÿæ˜¯ä¸ºäº†è§£é‡Šè¯´æ˜çš„ç›®çš„ã€‚å¦‚æœä½ æƒ³ï¼Œä½ ä¹Ÿå¯ä»¥å®‰å…¨åœ°è·³è¿‡æœ¬èŠ‚ã€‚

## é…ç½®å’Œæ•°æ®å‡†å¤‡


```python
!pip install -q diffusers
```

    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 503 kB 7.2 MB/s 
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182 kB 51.3 MB/s 
    [?25h


```python
import torch
import torchvision
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader
from diffusers import DDPMScheduler, UNet2DModel
from matplotlib import pyplot as plt
from tqdm.auto import tqdm

device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Using device: {device}')
```

    Using device: cuda



```python
# Load the dataset
dataset = torchvision.datasets.MNIST(root="mnist/", train=True, download=True, transform=torchvision.transforms.ToTensor())

# Feed it into a dataloader (batch size 8 here just for demo)
train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

# View some examples
x, y = next(iter(train_dataloader))
print('Input shape:', x.shape)
print('Labels:', y)
plt.imshow(torchvision.utils.make_grid(x)[0], cmap='Greys');
```

    Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist/MNIST/raw/train-images-idx3-ubyte.gz



      0%|          | 0/9912422 [00:00<?, ?it/s]


    Extracting mnist/MNIST/raw/train-images-idx3-ubyte.gz to mnist/MNIST/raw
    
    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist/MNIST/raw/train-labels-idx1-ubyte.gz



      0%|          | 0/28881 [00:00<?, ?it/s]


    Extracting mnist/MNIST/raw/train-labels-idx1-ubyte.gz to mnist/MNIST/raw
    
    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw/t10k-images-idx3-ubyte.gz



      0%|          | 0/1648877 [00:00<?, ?it/s]


    Extracting mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw
    
    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz



      0%|          | 0/4542 [00:00<?, ?it/s]


    Extracting mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw
    
    Input shape: torch.Size([8, 1, 28, 28])
    Labels: tensor([8, 1, 5, 9, 7, 6, 2, 2])



    
![png](02_class_conditioned_diffusion_model_example_CN_files/02_class_conditioned_diffusion_model_example_CN_4_9.png)
    


## åˆ›å»ºä¸€ä¸ªä»¥ç±»åˆ«ä¸ºæ¡ä»¶çš„ UNet

æˆ‘ä»¬è¾“å…¥ç±»åˆ«è¿™ä¸€æ¡ä»¶çš„æ–¹æ³•æ˜¯ï¼š
- åˆ›å»ºä¸€ä¸ªæ ‡å‡†çš„ `UNet2DModel`ï¼ŒåŠ å…¥ä¸€äº›é¢å¤–çš„è¾“å…¥é€šé“
- é€šè¿‡ä¸€ä¸ªåµŒå…¥å±‚ï¼ŒæŠŠç±»åˆ«æ ‡ç­¾æ˜ å°„åˆ°ä¸€ä¸ª `(class_emb_size)` å½¢çŠ¶çš„å­¦åˆ°çš„å‘é‡ä¸Š
- æŠŠè¿™ä¸ªä¿¡æ¯ä½œä¸ºé¢å¤–é€šé“å’ŒåŸæœ‰çš„è¾“å…¥å‘é‡æ‹¼æ¥èµ·æ¥ï¼Œç”¨è¿™è¡Œä»£ç ï¼š`net_input = torch.cat((x, class_cond), 1)`
- æŠŠè¿™ä¸ª `net_input` (æœ‰ `class_emb_size+1` ä¸ªé€šé“)è¾“å…¥åˆ°UNetä¸­å¾—åˆ°æœ€ç»ˆé¢„æµ‹

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘æŠŠ class_emb_size è®¾æˆ4ï¼Œä½†è¿™å…¶å®æ˜¯å¯ä»¥ä»»æ„ä¿®æ”¹çš„ï¼Œä½ å¯ä»¥è¯•è¯•ä»æŠŠå®ƒè®¾æˆ1ï¼ˆä½ å¯ä»¥çœ‹çœ‹è¿™æœ‰æ²¡æœ‰ç”¨ï¼‰åˆ°æŠŠå®ƒè®¾æˆ 10ï¼ˆæ­£å¥½æ˜¯ç±»åˆ«æ€»æ•°ï¼‰ï¼Œæˆ–è€…æŠŠéœ€è¦å­¦åˆ°çš„ nn.Embedding æ¢æˆç®€å•çš„å¯¹ç±»åˆ«è¿›è¡Œç‹¬çƒ­ç¼–ç (one-hot encodingï¼‰ã€‚

å…·ä½“å®ç°èµ·æ¥å°±æ˜¯è¿™æ ·ï¼š


```python
class ClassConditionedUnet(nn.Module):
  def __init__(self, num_classes=10, class_emb_size=4):
    super().__init__()
    
    # The embedding layer will map the class label to a vector of size class_emb_size
    self.class_emb = nn.Embedding(num_classes, class_emb_size)

    # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (the class embedding)
    self.model = UNet2DModel(
        sample_size=28,           # the target image resolution
        in_channels=1 + class_emb_size, # Additional input channels for class cond.
        out_channels=1,           # the number of output channels
        layers_per_block=2,       # how many ResNet layers to use per UNet block
        block_out_channels=(32, 64, 64), 
        down_block_types=( 
            "DownBlock2D",        # a regular ResNet downsampling block
            "AttnDownBlock2D",    # a ResNet downsampling block with spatial self-attention
            "AttnDownBlock2D",
        ), 
        up_block_types=(
            "AttnUpBlock2D", 
            "AttnUpBlock2D",      # a ResNet upsampling block with spatial self-attention
            "UpBlock2D",          # a regular ResNet upsampling block
          ),
    )

  # Our forward method now takes the class labels as an additional argument
  def forward(self, x, t, class_labels):
    # Shape of x:
    bs, ch, w, h = x.shape
    
    # class conditioning in right shape to add as additional input channels
    class_cond = self.class_emb(class_labels) # Map to embedding dinemsion
    class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h)
    # x is shape (bs, 1, 28, 28) and class_cond is now (bs, 4, 28, 28)

    # Net input is now x and class cond concatenated together along dimension 1
    net_input = torch.cat((x, class_cond), 1) # (bs, 5, 28, 28)

    # Feed this to the unet alongside the timestep and return the prediction
    return self.model(net_input, t).sample # (bs, 1, 28, 28)
```

å¦‚æœä½ å¯¹ä»»ä½•çš„å¼ é‡å½¢çŠ¶æˆ–å˜æ¢æ„Ÿåˆ°è¿·æƒ‘ï¼Œä½ éƒ½å¯ä»¥åœ¨ä»£ç ä¸­åŠ å…¥printæ¥çœ‹çœ‹ç›¸å…³å½¢çŠ¶ï¼Œæ£€æŸ¥ä¸€ä¸‹æ˜¯ä¸æ˜¯å’Œä½ é¢„è®¾çš„æ˜¯ä¸€è‡´çš„ã€‚è¿™é‡Œæˆ‘æŠŠä¸€äº›ä¸­é—´å˜é‡çš„å½¢çŠ¶éƒ½æ³¨é‡Šä¸Šäº†ï¼Œå¸Œæœ›èƒ½å¸®ä½ æ€è·¯æ¸…æ™°ç‚¹ã€‚

## è®­ç»ƒå’Œé‡‡æ ·

ä¸åŒäºåˆ«çš„åœ°æ–¹ä½¿ç”¨çš„`prediction = unet(x, t)`ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨`prediction = unet(x, t, y)`ï¼Œåœ¨è®­ç»ƒæ—¶æŠŠæ­£ç¡®çš„æ ‡ç­¾ä½œä¸ºç¬¬ä¸‰ä¸ªè¾“å…¥é€åˆ°æ¨¡å‹ä¸­ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬å¯ä»¥è¾“å…¥ä»»ä½•æˆ‘ä»¬æƒ³è¦çš„æ ‡ç­¾ï¼Œå¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œé‚£æ¨¡å‹å°±ä¼šè¾“å‡ºä¸ä¹‹åŒ¹é…çš„å›¾ç‰‡ã€‚`y`åœ¨è¿™é‡Œæ—¶ MNIST ä¸­çš„æ•°å­—æ ‡ç­¾ï¼Œå€¼çš„èŒƒå›´ä»0åˆ°9ã€‚

è¿™é‡Œçš„è®­ç»ƒå¾ªç¯å¾ˆåƒ[ç¬¬ä¸€å•å…ƒçš„ä¾‹å­](../unit1/02_diffusion_models_from_scratch_CN.ipynb)ã€‚æˆ‘ä»¬è¿™é‡Œé¢„æµ‹çš„æ˜¯å™ªå£°ï¼ˆè€Œä¸æ˜¯åƒç¬¬ä¸€å•å…ƒçš„å»å™ªå›¾ç‰‡ï¼‰ï¼Œä»¥æ­¤æ¥åŒ¹é… DDPMScheduler é¢„è®¡çš„ç›®æ ‡ã€‚è¿™é‡Œæˆ‘ä»¬ç”¨ DDPMScheduler æ¥åœ¨è®­ç»ƒä¸­åŠ å™ªå£°ï¼Œå¹¶åœ¨æ¨ç†æ—¶é‡‡æ ·ç”¨ã€‚è®­ç»ƒä¹Ÿéœ€è¦ä¸€æ®µæ—¶é—´ â€”â€” å¦‚ä½•åŠ é€Ÿè®­ç»ƒä¹Ÿå¯ä»¥æ˜¯ä¸ªæœ‰è¶£çš„å°é¡¹ç›®ã€‚ä½†ä½ ä¹Ÿå¯ä»¥è·³è¿‡è¿è¡Œä»£ç ï¼ˆç”šè‡³æ•´èŠ‚ç¬”è®°æœ¬ï¼‰ï¼Œå› ä¸ºæˆ‘ä»¬è¿™é‡Œçº¯ç²¹æ˜¯åœ¨è®²è§£æ€è·¯ã€‚


```python
# Create a scheduler
noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2')
```


```python
#@markdown Training loop (10 Epochs):

# Redefining the dataloader to set the batch size higher than the demo of 8
train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True)

# How many runs through the data should we do?
n_epochs = 10

# Our network 
net = ClassConditionedUnet().to(device)

# Our loss finction
loss_fn = nn.MSELoss()

# The optimizer
opt = torch.optim.Adam(net.parameters(), lr=1e-3) 

# Keeping a record of the losses for later viewing
losses = []

# The training loop
for epoch in range(n_epochs):
    for x, y in tqdm(train_dataloader):
        
        # Get some data and prepare the corrupted version
        x = x.to(device) * 2 - 1 # Data on the GPU (mapped to (-1, 1))
        y = y.to(device)
        noise = torch.randn_like(x)
        timesteps = torch.randint(0, 999, (x.shape[0],)).long().to(device)
        noisy_x = noise_scheduler.add_noise(x, noise, timesteps)

        # Get the model prediction
        pred = net(noisy_x, timesteps, y) # Note that we pass in the labels y

        # Calculate the loss
        loss = loss_fn(pred, noise) # How close is the output to the noise

        # Backprop and update the params:
        opt.zero_grad()
        loss.backward()
        opt.step()

        # Store the loss for later
        losses.append(loss.item())

    # Print our the average of the last 100 loss values to get an idea of progress:
    avg_loss = sum(losses[-100:])/100
    print(f'Finished epoch {epoch}. Average of the last 100 loss values: {avg_loss:05f}')

# View the loss curve
plt.plot(losses)
```


      0%|          | 0/469 [00:00<?, ?it/s]


    Finished epoch 0. Average of the last 100 loss values: 0.052451



      0%|          | 0/469 [00:00<?, ?it/s]


    Finished epoch 1. Average of the last 100 loss values: 0.045999



      0%|          | 0/469 [00:00<?, ?it/s]


    Finished epoch 2. Average of the last 100 loss values: 0.043344



      0%|          | 0/469 [00:00<?, ?it/s]


    Finished epoch 3. Average of the last 100 loss values: 0.042347



      0%|          | 0/469 [00:00<?, ?it/s]


    Finished epoch 4. Average of the last 100 loss values: 0.041174



      0%|          | 0/469 [00:00<?, ?it/s]


    Finished epoch 5. Average of the last 100 loss values: 0.040736



      0%|          | 0/469 [00:00<?, ?it/s]


    Finished epoch 6. Average of the last 100 loss values: 0.040386



      0%|          | 0/469 [00:00<?, ?it/s]


    Finished epoch 7. Average of the last 100 loss values: 0.039372



      0%|          | 0/469 [00:00<?, ?it/s]


    Finished epoch 8. Average of the last 100 loss values: 0.039056



      0%|          | 0/469 [00:00<?, ?it/s]


    Finished epoch 9. Average of the last 100 loss values: 0.039024





    [<matplotlib.lines.Line2D>]




    
![png](02_class_conditioned_diffusion_model_example_CN_files/02_class_conditioned_diffusion_model_example_CN_10_21.png)
    


ä¸€æ—¦è®­ç»ƒç»“æŸï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡è¾“å…¥ä¸åŒçš„æ ‡ç­¾ä½œä¸ºæ¡ä»¶ï¼Œæ¥é‡‡æ ·å›¾ç‰‡äº†ï¼š


```python
#@markdown Sampling some different digits:

# Prepare random x to start from, plus some desired labels y
x = torch.randn(80, 1, 28, 28).to(device)
y = torch.tensor([[i]*8 for i in range(10)]).flatten().to(device)

# Sampling loop
for i, t in tqdm(enumerate(noise_scheduler.timesteps)):

    # Get model pred
    with torch.no_grad():
        residual = net(x, t, y)  # Again, note that we pass in our labels y

    # Update sample with step
    x = noise_scheduler.step(residual, t, x).prev_sample

# Show the results
fig, ax = plt.subplots(1, 1, figsize=(12, 12))
ax.imshow(torchvision.utils.make_grid(x.detach().cpu().clip(-1, 1), nrow=8)[0], cmap='Greys')
```


    0it [00:00, ?it/s]





    <matplotlib.image.AxesImage>




    
![png](02_class_conditioned_diffusion_model_example_CN_files/02_class_conditioned_diffusion_model_example_CN_12_2.png)
    


å°±æ˜¯è¿™ä¹ˆç®€å•ï¼æˆ‘ä»¬ç°åœ¨å·²ç»å¯¹è¦ç”Ÿæˆçš„å›¾ç‰‡æœ‰æ‰€æ§åˆ¶äº†ã€‚

å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªä¾‹å­ã€‚ä¸€å¦‚æ—¢å¾€åœ°ï¼Œå¦‚æœä½ æœ‰é—®é¢˜ï¼Œä½ éšæ—¶å¯ä»¥åœ¨ Discord ä¸Šæå‡ºæ¥ã€‚


```python
# ç»ƒä¹ ï¼ˆé€‰åšï¼‰ï¼šç”¨åŒæ ·æ–¹æ³•åœ¨ FashionMNIST æ•°æ®é›†ä¸Šè¯•è¯•ã€‚è°ƒèŠ‚å­¦ä¹ ç‡ã€batch size å’Œè®­ç»ƒçš„è½®æ•°ï¼ˆepochsï¼‰ã€‚
# ä½ èƒ½ç”¨æ¯”ä¾‹å­æ›´å°‘çš„è®­ç»ƒæ—¶é—´å¾—åˆ°äº›çœ‹èµ·æ¥ä¸é”™çš„æ—¶å°šç›¸å…³çš„å›¾ç‰‡å—ï¼Ÿ
```
